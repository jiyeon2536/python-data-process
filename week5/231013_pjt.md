# 파이썬으로 웹 페이지에 있는 정보를 가져오는 방법
1. 누군가 업로드해둔 데이터를 다운로드 받기 (캐글)
2. 누군가 만들어둔 API server를 활용하여 정보를 받아오기
3. 사람이 검색하는 것처럼 파이썬이 자동으로 검색 후 결과를 수집
    - 크롤링

# 데이터 사이언스 프로세스
- 필요한 정보를 추출하는 5가지 단계
1. 문제 정의 : 해결하고자 하는 문제 정의
2. 데이터 수집 : 문제 해결에 필요한 데이터 수집
3. 데이터 전처리(정제) : 실질적인 분석을 수해하기 위해 데이터를 가공하는 단계
    - 수집한 데이터의 오류 제거 (결측치, 이상치), 데이터 형식 변환 
4. 데이터 분석 : 전처리가 완료된 데이터에서 필요한 정보를 추출하는 단계
5. 결과 해석 및 공유 : 의사 결정에 활용하기 위해 결과를 해석하고 시각화 후 공유하는 단계


## 데이터 수집
1. 웹 스크래핑 : 웹 페이지에서 데이터를 추출하는 기술
2. 웹 크롤링: 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술
3. open api 활용 : 공개된 api를 통해 데이터 수집
4. 데이터 공유 플랫폼 활용: 다양한 사용자가 데이터를 공유하고 활용할 수 있는 온라인 플랫폼
    - 캐글, 데이터.월드, 데이콘, 공공데이터포털 등

### 웹 크롤링이란?
- 여러 웹 페이지를 돌아다니며 원하는 정보를 모으는 기술
- 원하는 정보를 추출하는 스크래핑과 여러 웹 페이지를 자동으로 탐색하는 크롤링의 개념을 합쳐 웹 크롤링이라고 부름
- 웹사이트들을 돌아다니며 필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스

### 웹 크롤링 프로세스
- 웹 페이지 다운로드
    - 해당 웹페이지의 HTML, CSS, JavaScript 등의 코드를 가져오는 단계
- 페이지 파싱
    - 다운로드 받은 코드를 분석하고 필요한 데이터를 추출하는 단계
- 링크 추출 및 다른 페이지 탐색
    - 다른 링크를 추출하고 다음 단계로 이동하여 원하는 데이터를 추출하는 단계
- 데이터 추출 및 저장
    - 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장하는 단계


### 실습
- 필요 라이브러리
    - requests : HTTP 요청을 보내고 응답을 받을 수 있는 모듈
    - beautifulsoup : HTML 문서에서 원하는 데이터를 추출하는 데 사용되는 파이썬 라이브러리
    - selenium : 웹 애플리케이션을 테스트하고 자동화하기 위한 파이썬 라이브러리
        - 웹 페이지의 동적인 컨텐츠를 가져오기 위해 사용함(검색 결과 등)


- beautifulsoup 공식 문서
https://tedboy.github.io/bs4_doc/


### selenium 
-자바스크립트가 모두 동작한 후의 최종 결과물을 받아올 때사용

/robots.txt
을 쳐서 크롤링 해도 되는 사항 안되는 사항을 확인할 수 있다
google.com/robots.txt